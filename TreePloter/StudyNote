决策树
  通常使用决策树处理分类问题
  kNN算法可以完成很多分类任务，但是它最大的缺点就是无法给出数据的内在含义，决策树的主要优势就在于数据形式非常容易理解

优点：
  计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不想管的特征数据
缺点：
  可能会产生过度匹配的问题
适用数据类型：数值型和标称型

决策树的一般流程：
  1、收集数据： 可以使用任何方法
  2、准备数据： 树构造算法只适用于标称型数据，因此数值型数据必须离散化
  3、分析数据： 可以使用任何方法，构造树完成后，我们应该检查图形是否符合预期。
  4、训练算法： 构造树的数据结构
  5、测试算法： 使用经验树计算误差率
  6、使用算法： 此步骤适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义
  
在构造决策树时，需要解决第一个问题就是
    当前数据集上哪个特征在划分数据分类时起决定性作用
    所以我们首先必须评估每个特征

    创建分支的为代码函数createBranch():
        if so return 类标签;
        else:
            寻中划分数据集的最好特征
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数createBranch()并增加返回结果到分支节点中
            return 分支节点

    采用ID3算法划分数据集

    划分数据集的大原则是：将无序的数据变得更加有序
    在划分数据集之前之后信息发生的变化称为信息增益
    计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择
  
 
